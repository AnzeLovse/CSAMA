% rm(list=ls()); library('knitr'); pat_md();
% options(knitr.duplicate.label = 'allow');knit('Supervised.Rnw'); options(prompt="> ");

%-------------------------------------------------
\chapter{Supervised Learning}
\label{Chap:16}\label{Chap:Supervised}
%--------------------------------------------------
```{r initialize, echo = FALSE, message = FALSE, error = FALSE, warning = FALSE}
source("../chapter-setup.R"); chaptersetup("Supervised.Rnw", "16")
```

\begin{marginfigure}[-2cm]
\includegraphics[width=0.8\textwidth]{BuildWall.png}
\vskip1cm
\includegraphics[width=0.8\textwidth]{EWall.png}
\caption{In a supervised learning setting, we have a yardstick or plumbline to judge how
  well we are doing: the response itself.}
\end{marginfigure}

A frequent question in biological and biomedical applications is whether a property of
interest (say, disease type, cell type, the prognosis of a patient) can be ``predicted'',
given one or more other properties, called the \eindex[predictor]{predictors}. Often we
are motivated by a situation in which the property to be predicted is unknown (it lies in
the future, or is hard to measure), while the predictors are known. The crucial point is
that we \emph{learn} the prediction rule from a set of training
data in which the property of interest is also known. Once we have the rule, we can either
apply it to new data, and make actual predictions of unknown outcomes; or we can dissect
the rule with the aim of better understanding the underlying biology.

Compared to unsupervised learning and what we have seen in Chapters~\ref{Chap:6},
\ref{Chap:8} and \ref{Chap:9}, where we do not know what we are looking for or how to
decide whether our result is ``right'', we are on much more solid ground with supervised learning: the
objective is clearly stated, and there are straightforward criteria to measure how well we are
doing.

The central issue in \eindex{supervised learning}\footnote{Sometimes the term
  \eindex{statistical learning} is used, more or less exchangeably.}  is
\eindex{overfitting} and \eindex{generalisability}: did we just learn the training data
``by heart'' by constructing a rule that has 100\% accuracy on the training data, but
would perform poorly on any new data?  Or did our rule indeed pick up some of the
pertinent patterns in the system being studied, which will also apply to yet unseen new
data?

%-------------------------------------
\section{Goals for this chapter}
%-------------------------------------
In this chapter we will
\begin{itemize}
\item see exemplary applications that motivate the use of supervised learning methods
\item learn what discriminant analysis does,
\item define measures of performance,
\item encounter the curse of dimensionality and see what overfitting is,
\item find out about regularisation and understand the concepts of generisability and model complexity,
\item see how to use cross-validation to tune parameters of the algorithms,
\item get to see a unified framework for machine learning algorithms in R that allows you to
  use hundreds of methods in a consistent manner,
\item discuss method hacking.
\end{itemize}

%------------------------------------------------------
\section{What are the data?}
%------------------------------------------------------
The basic data structure for both supervised and unsupervised learning is (at least
conceptually) a dataframe, where each row corresponds to an object and the columns are
different features\footnote{Features are usually numerical scalars or categorical variables,
  although some methods can be generalized to work with other data types.} of
the objects. While in unsupervised learning we aim to find (dis)similarity relationships
between the objects based on their feature values (e.\,g., by clustering or ordination),
in supervised learning we aim to find a mathematical function (or a computational algorithm)
that predicts the value of one of the features from the other features.  Many
implementations require that there are no missing values, whereas other methods can be
generalized to work with some amount of missing data.

The feature that we select over all the others with the aim of predicting is called the
\eindex{objective} or the \eindex{response}.  Sometimes the choice is natural, but sometimes it is also
instructive to reverse the roles, especially if we are interested in dissecting the
prediction function for the purpose of biological understanding, or in disentangling
correlations from causation.

The framework for supervised learning covers both continuous and categorical response
variables. In the continuous case we also call it \eindex{regression}, in the
categorical case, \eindex{classification}. It turns out that this distinction is not a
detail, as it has quite far-reaching consequences for the choice of loss function
(Section~\ref{sec:supervised:objective}) and thus the choice of algorithm~\citep{friedmanbiasvariance01}.

The first question to consider in any supervised learning task is how the number of
objects compares to the number of predictors. The more data, the better, and much of the
hard work in supervised learning has to do with overcoming the limitations of having a
finite (and typically, too small) training set.

\begin{marginfigure}
\includegraphics[width=\textwidth]{fourquad.pdf}
\label{fig:fourtypes}
\caption{In supervised learning, we assign two different roles to our variables. We have
  labeled the explanatory variables $X$ and the response variable(s) $Y$.  There are also
  two different sets of observations: the training set $X_\ell$ and $Y_\ell$ and the
  validation set $X_v$ and $Y_v$.}
\end{marginfigure}

\begin{ques}
  Give examples where we have encountered instances of supervised learning with a
  categorical response in this book.
\end{ques}

%------------------------------------------------------------
\subsection{Motivating examples}
%------------------------------------------------------------
\subsubsection{Predicting diabetes type}
%------------------------------------------------------------
The \Robject{diabetes} dataset~\citep{diabetes} presents three different groups of diabetes
patients and five clinical variables measured on them.
%
```{r diabetes}
library("ggplot2")
library("readr")
library("magrittr")
diabetes = read_csv("../data/diabetes.csv", col_names = TRUE)
diabetes
diabetes$group %<>% factor
```
%$
We used the forward-backward pipe operator \Rfunction{\%$<>$\%} to convert the
\Robject{group} column into a factor.
%
```{r ldagroups, fig.width = 4.5, fig.height = 4.5}
library("reshape2")
ggplot(melt(diabetes, id.vars = c("id", "group")),
       aes(x = value, col = group)) +
  geom_density() + facet_wrap( ~variable, ncol = 2, scales = "free")
```
\begin{marginfigure}
\includegraphics[width=\textwidth]{chap16-r_ldagroups-1}
\caption{We see already from the one-dimensional distributions that some of the individual
  variables could potentially predict which group a patient is more likely to belong
  to. Our goal will be to combine variables to improve these one dimensional predictions.}
\label{chap16-r_ldagroups-1}
\end{marginfigure}
The plot is shown in Figure~\ref{chap16-r_ldagroups-1}.

%------------------------------------------------------------
\subsubsection{Predicting cellular phenotypes}
%------------------------------------------------------------
\citet{Neumann:2010} observed human cancer cells using live-cell imaging. The cells were genetically
engineered so that their histones were tagged with a green fluorescent protein (GFP).  A genome-wide
RNAi library was applied to the cells, and for each siRNA perturbation, movies of a few
hundred cells were recorded for about two days, to see what effect the depletion of each
gene had on cell cycle, nuclear morphology and cell proliferation.  Their paper reports the
use of an automated image classification algorithm that quantified the visual appearance of each
cell's nucleus and enabled the prediction of normal mitosis states or aberrant nuclei.
The algorithm was trained on the data from around 3000 cells that were annotated by a
human expert. It was then applied to almost 2 billions images of nuclei
(Figure~\ref{fig:supervised:cellshape}). Using automated image
classification provided scalablity (annotating 2 billion images manually would take a
long time) and objectivity.
%
\begin{figure}
\begin{center}
\includegraphics[width=0.7\textwidth]{Neumann2010Fig1b.png}
\caption{The data were images of $2x10^9$ nuclei from movies. The images were segmented to
  identify the nuclei, and numeric features were computed for each nucleus, corresponding
  to size, shape, brightness and lots of other more or less abstract quantitative
  summaries of the joint distribution of pixel intensities. From the features, the cells
  were classified into 16 different nuclei morphology classes, represented by the rows of
  the barplot. Representative images for each class are shown in black and white in
  the center column. The class frequencies, which are very unbalanced, are shown by the
  lengths of the bars.}
\label{fig:supervised:cellshape}
\end{center}
\end{figure}

%------------------------------------------------------------
\subsubsection{Predicting embryonic cell states}
%------------------------------------------------------------
We will revisit the mouse embryo data~\citep{Ohnishi2014}, which we have already seen in
Chapters~\ref{Chap:Graphics}, \ref{Chap:Clustering} and \ref{Chap:Multivariate},
and show how we can predict the developmental state (Embryonic Days) from the gene expression measurements.

%---------------------------------------------
\section{Linear discrimination}
%---------------------------------------------
We start with one of the simplest possible discrimination problems\footnote{Arguably the
  simplest possible problem is a single continuous feature, two classes, and the task of
  finding a single threshold to discriminate between the two groups.}, where we have
objects described by two continuous features (so the objects can be thought of as points
in the 2D plane) and falling into three groups. Our aim is to define class boundaries,
which are lines in the 2D space.

%---------------------------------------------
\subsection{Diabetes data}
%---------------------------------------------
Let's see whether we can predict the feature \Robject{group} from the features \Robject{insulin} and
\Robject{glutest} variables in the \Robject{diabetes} data. It's always a good idea to
first visualise the data (Figure~\ref{chap16-r_scatterdiabetes-1}).
%
```{r scatterdiabetes, fig.width = 3.5, fig.height = 3}
ggdb = ggplot(mapping = aes(x = insulin, y = glutest)) +
  geom_point(aes(colour = group), data = diabetes)
ggdb
```
%
\begin{marginfigure}[-1cm]
\includegraphics[width=\textwidth]{chap16-r_scatterdiabetes-1}
\caption{Scatterplot of two of the variables in the \Robject{diabetes} data. Each point is a sample, and the
color indicates the diabetes type as encoded in the \Robject{group} variable.}
\label{chap16-r_scatterdiabetes-1}
\end{marginfigure}
%
We'll start with a method
called \eindex{linear discriminant analysis} (\eindex{LDA}). This method is a foundation
stone of classification, many of the more complicated (and sometimes more powerful)
algorithms are really just generalisations of LDA.
%
```{r ldaresults}
library("MASS")
diabetes_lda = lda(group ~ insulin + glutest, data = diabetes)
diabetes_lda
ghat = predict(diabetes_lda)$class
table(ghat, diabetes$group)
mean(ghat != diabetes$group)
```
%$
\begin{ques}
  What do the different parts of the above output mean?
\end{ques}
%
Now, let's visualise the LDA result\footnote{Note how we first visualised the data, in
  Figure~\ref{chap16-r_scatterdiabetes-1}, and are now going to visualise the fitted model
  (Figure~\ref{chap16-r_modeldiabetes-1}). The prediction regions can, in principle, be
  shown for any classification method, including a ``black box'' method. On the other hand, the
  cluster centers and ellipses in Figure~\ref{chap16-r_modeldiabetes-1} are a method-specific
  visualisation.}.  We are going to plot the prediction
regions for each of the three groups.  We do this by creating a grid of points and using
our prediction rule on each of them. We'll then also dig a bit deeper into the mechanics of
LDA and plot the class centers (\Robject{diabetes\_lda\$means}) and ellipses that
correspond to the fitted covariance matrix (\Robject{diabetes\_lda\$scaling}). Assembling this
visualization requires us to write a bit of code.
%
```{r make1Dgrid}
make1Dgrid = function(x) {
  rg  = range(x)
  wid = diff(rg)
  rg  = rg + wid * 0.05 * c(-1, 1)
  seq(from = rg[1], to = rg[2], length.out = 100)
}
```
%
Set up the points for prediction, a $100 \times 100$ grid that covers the data range.
%
```{r diabetes_grid}
diabetes_grid = with(diabetes,
  expand.grid(insulin = make1Dgrid(insulin),
              glutest = make1Dgrid(glutest)))
```
%
Do the predictions.
%
```{r diabetes_grid}
diabetes_grid$ghat =
  predict(diabetes_lda, newdata = diabetes_grid)$class
```
%$
The group centers.
```{r centers}
centers = diabetes_lda$means
```
%$
Compute a unit circle and an affine transformation of the circle into the ellipse we want to plot.
```{r unitcircle}
unitcircle = exp(1i * seq(0, 2*pi, length.out = 90)) %>%
          (function(x) cbind(Re(x), Im(x)))
ellipse = unitcircle %*% solve(diabetes_lda$scaling)
```
All three ellipses, one for each group center.
```{r ellipses}
ellipses = lapply(seq_len(nrow(centers)), function(i) {
  (ellipse +
   matrix(centers[i, ], byrow = TRUE,
          ncol = ncol(centers), nrow = nrow(ellipse))) %>%
     cbind(group = i)
}) %>% do.call(rbind, .) %>% data.frame
ellipses$group %<>% factor
```
%$
Now we are ready to plot (Figure~\ref{chap16-r_modeldiabetes-1}).
%
```{r modeldiabetes, fig.width = 5, fig.height = 4}
ggdb + geom_raster(aes(fill = ghat),
            data = diabetes_grid, alpha = 0.4, interpolate = TRUE) +
    geom_point(data = as_data_frame(centers), pch = "+", size = 8) +
    geom_path(aes(colour = group), data = ellipses) +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0))
```
%
\begin{marginfigure}
\includegraphics[width=\textwidth]{chap16-r_modeldiabetes-1}
\caption{As Figure~\ref{chap16-r_scatterdiabetes-1}, with the classification regions from the LDA model shown.}
\label{chap16-r_modeldiabetes-1}
\end{marginfigure}
%
\begin{ques}
  Why is the boundary between the prediction regions for groups 1 and 2 not perpendicular
  to the line between the cluster centers?
\end{ques}
\begin{ques}
  How confident would you be about the predictions in those areas of the 2D plane that are
  far from all of the cluster centers?
\end{ques}
\begin{ques}
  Why is the boundary between the prediction regions for groups 2 and 3 not half-way between the centers, but
  shifted in favor of class 3?  (Hint: have a look at the \Robject{prior} argument of
  \Rfunction{lda}.) Try again with uniform prior.
\end{ques}
\begin{answer}
The result of the following code chunk is shown in Figure~\ref{chap16-r_diabetes_lda_uniform_prior-1}.
\begin{marginfigure}
\includegraphics[width=\textwidth]{chap16-r_diabetes_lda_uniform_prior-1}
\caption{As Figure~\ref{chap16-r_modeldiabetes-1}, but with uniform class priors.}
\label{chap16-r_diabetes_lda_uniform_prior-1}
\end{marginfigure}
%
```{r diabetes_lda_uniform_prior, fig.width = 5, fig.height = 4}
diabetes_up = lda(group ~ insulin + glutest, data = diabetes,
  prior = with(diabetes, rep(1/nlevels(group), nlevels(group))))

diabetes_grid$ghup =
  predict(diabetes_up, newdata = diabetes_grid)$class

stopifnot(all.equal(diabetes_up$means, diabetes_lda$means))

ellipse_up  = unitcircle %*% solve(diabetes_up$scaling)
ellipses_up = lapply(seq_len(nrow(centers)), function(i) {
  (ellipse_up +
   matrix(centers[i, ], byrow = TRUE,
          ncol = ncol(centers), nrow = nrow(ellipse_up))) %>%
     cbind(group = i)
}) %>% do.call(rbind, .) %>% data.frame
ellipses_up$group %<>% factor

ggdb + geom_raster(aes(fill = ghup),
            data = diabetes_grid, alpha = 0.4, interpolate = TRUE) +
    geom_point(data = data.frame(centers), pch = "+", size = 8) +
    geom_path(aes(colour = group), data = ellipses_up) +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0))
```
%
The \Rfunction{stopifnot} line confirms that the class centers are the same
--they are independent of the prior--, but the joint covariance is not.
\end{answer}

\begin{ques}
  What is the difference in the prediction accuracy if we use all 5 variables instead of
  just \Robject{insulin} and \Robject{glufast}?
\end{ques}

\begin{answer}
~~
```{r all5diab}
diabetes_lda5 = lda(group ~ relwt + glufast + glutest +
           steady + insulin, data = diabetes)
diabetes_lda5
ghat5 = predict(diabetes_lda5)$class
table(ghat5, diabetes$group)
mean(ghat5 != diabetes$group)
```
\end{answer}

\begin{ques}
  Instead of approximating the prediction regions by  classification from a grid of points,
  compute the separating lines explicitly from the linear determinant coefficients.
\end{ques}
\begin{answer}
See Section 4.3, Equation (4.10) in \citep{HastieTibshiraniFriedman}.
\end{answer}

%----------------------------------------------------------------------
\subsection{Predicting embryonic cell state from gene expression}
%----------------------------------------------------------------------
%
Assume that we already know that the four genes \textit{FN1}, \textit{TIMD2}, \textit{GATA4} and \textit{SOX7}
are relevant to the classification task\footnote{Later in this chapter we will see methods that can drop this assumption
and screen all available features.}. We want to build a classifier that predict the
developmental time (embryonic days, E3.25, E3.5, E4.5). We load the data and select four corresponding probes.
%
```{r loadHiiragi2}
library("Hiiragi2013")
data("x")
probes = c("1426642_at","1418765_at","1418864_at","1416564_at")
embryoCells = as_data_frame(t(exprs(x)[probes, ])) %>%
  mutate(Embryonic.day = x$Embryonic.day) %>%
  filter(x$genotype == "WT")
```
%
We can use the Bioconductor annotation package associated with the microarray to verify that the probes correspond
to the intended genes,
%
```{r annoHiiragi}
annotation(x)
library("mouse4302.db")
anno = AnnotationDbi::select(mouse4302.db, keys = probes,
         columns = c("SYMBOL", "GENENAME"))
anno
mt = match(anno$PROBEID, colnames(embryoCells))
colnames(embryoCells)[mt] = anno$SYMBOL
```
```{r assertprobeid, echo = FALSE}
stopifnot(!any(is.na(mt)))
```
%
and produce a pairs plot (Figure~\ref{chap16-HiiragiFourGenesPairs-1}).
%
```{r HiiragiFourGenesPairs, fig.width = 6, fig.height = 6}
library("GGally")
ggpairs(embryoCells, mapping = aes(col = Embryonic.day),
  columns = anno$SYMBOL, upper = list(continuous = "points"))
```
%
\begin{figure}
\begin{center}
\includegraphics[width=0.9\textwidth]{chap16-r_HiiragiFourGenesPairs-1}
\caption{Expression values of the discriminating genes, with the prediction target
  Embryonic.day shown by color.}
\label{chap16-HiiragiFourGenesPairs-1}
\end{center}
\end{figure}
%
We can now call \Rfunction{lda} on these data.
The linear combinations \Robject{LD1} and \Robject{LD2} that serve as discriminating variables are
given in the slot \Robject{ed\_lda\$scaling} of the output from \Rfunction{lda}.
%
```{r ldacells, fig.width=8, fig.height=4}
ec_lda = lda(Embryonic.day ~ Fn1 + Timd2 + Gata4 + Sox7,
             data = embryoCells)
round(ec_lda$scaling, 1)
```
%
For the visualisation of the learned model in Figure~\ref{chap16-r_edcontour-1}, we need to
build the prediction regions and their boundaries by expanding the grid in the space of the
two new coordinates LD1 and LD2.
%
```{r edcontour, fig.width = 4.5, fig.height = 3.5}
ec_rot = predict(ec_lda)$x %>% as_data_frame %>%
           mutate(ed = embryoCells$Embryonic.day)

ec_lda2 = lda(ec_rot[, 1:2], predict(ec_lda)$class)

ec_grid = with(ec_rot, expand.grid(
  LD1 = make1Dgrid(LD1),
  LD2 = make1Dgrid(LD2)))

ec_grid$edhat = predict(ec_lda2, newdata = ec_grid)$class

ggplot() +
  geom_point(aes(x = LD1, y = LD2, colour = ed), data = ec_rot) +
  geom_raster(aes(x = LD1, y = LD2, fill = edhat),
            data = ec_grid, alpha = 0.4, interpolate = TRUE) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  coord_fixed()
```
%
\begin{marginfigure}
\includegraphics[width=\textwidth]{chap16-r_edcontour-1}
\caption{LDA classification regions for Embryonic.day.}
\label{chap16-r_edcontour-1}
\end{marginfigure}

\begin{ques}
Repeat these analyses using quadratic discriminant analysis (\Rfunction{qda}).
What difference do you see in the shape of the boundaries?
\end{ques}
\begin{answer}
See Figure~\ref{chap16-r_qdamouse-1}.
```{r qdamouse, fig.width = 9, fig.height = 5.8}
library("gridExtra")

ec_qda = qda(Embryonic.day ~ Fn1 + Timd2 + Gata4 + Sox7,
             data = embryoCells)

variables = colnames(ec_qda$means)
pairs = combn(variables, 2)
lapply(seq_len(ncol(pairs)), function(i) {
  grid = with(embryoCells,
    expand.grid(x = make1Dgrid(get(pairs[1, i])),
                y = make1Dgrid(get(pairs[2, i])))) %>%
    `colnames<-`(pairs[, i])

  for (v in setdiff(variables, pairs[, i]))
    grid[[v]] = median(embryoCells[[v]])

  grid$edhat = predict(ec_qda, newdata = grid)$class

  ggplot() + geom_point(
      aes_string(x = pairs[1, i], y = pairs[2, i],
      colour = "Embryonic.day"), data = embryoCells) +
    geom_raster(
      aes_string(x = pairs[1, i], y = pairs[2, i], fill = "edhat"),
      data = grid, alpha = 0.4, interpolate = TRUE) +
    scale_x_continuous(expand = c(0, 0)) +
    scale_y_continuous(expand = c(0, 0)) +
    coord_fixed() +
    if (i != ncol(pairs)) theme(legend.position = "none")
}) %>% grid.arrange(grobs = ., ncol = 3)
```
\begin{figure}
\begin{center}
\includegraphics[width=0.85\textwidth]{chap16-r_qdamouse-1}
\caption{QDA for the mouse cell data, all pairwise plots of the four features.}
\label{chap16-r_qdamouse-1}
\end{center}
\end{figure}
\end{answer}

\begin{ques}
  What happens if you call \Rfunction{lda} or \Rfunction{qda} with a lot more genes, say
  the first 1000, in the Hiiragi dataset?
\end{ques}
%----
\begin{answer}
~~
```{r ladallvariables, warning = TRUE, error = TRUE, results = "hide"}
lda(t(exprs(x))[, 1:1000], x$Embryonic.day)
qda(t(exprs(x))[, 1:1000], x$Embryonic.day)
```
\end{answer}

%------------------------------------------------------------
\section{Machine learning vs rote learning}
%------------------------------------------------------------
Computers are really good at memorizing facts. In the worst case, a machine learning
algorithm is a roundabout way of doing this\footnote{The not so roundabout way is
  database technologies.}.  The central question in statistical learning is whether the
algorithm was able to generalize, i.\,e., interpolate and extrapolate. Let's look at the
following example. We generate random data (\Rfunction{rnorm}) for \Robject{n} objects,
with different numbers of features (given by \Robject{p}). We train a LDA on these data
and compute the \eindex{misclassification rate}, i.\,e., the fraction of times the
prediction is wrong (\Robject{pred != resp}).
%
```{r learnbyheart, warning = FALSE}
library("dplyr")
p = 2:21
n = 20

mcl = lapply(p, function(k) {
  replicate(100, {
    xmat = matrix(rnorm(n * k), nrow = n)
    resp = sample(c("apple", "orange"), n, replace = TRUE)
    fit  = lda(xmat[, 1:k], resp)
    pred = predict(fit)$class
    mean(pred != resp)
  }) %>% mean %>% tibble(mcl = .)
}) %>% bind_rows %>% cbind(., p = p)

ggplot(mcl, aes(x = p, y = mcl)) + geom_line() + geom_point() +
  ylab("Misclassification rate")
```
%$
\begin{marginfigure}
\includegraphics[width=\textwidth]{chap16-r_learnbyheart-1}
\caption{Misclassification rate of LDA applied to random data.  With increasing number of
  features (\Robject{p}), the misclassification rate becomes almost zero as \Robject{p}
  approaches \Robject{n}, the number of objects. (As \Robject{p} becomes even larger, the
  "performance" degrades again, apparently due to numerical properties of the
  \Rfunction{lda} implementation used here.)}
\label{chap16-r_learnbyheart-1}
\end{marginfigure}
%
\begin{ques}
  What is the purpose of the \Rfunction{replicate} loop in the above code? What happens if
  you omit it (or replace the 100 by 1)?
\end{ques}
\begin{answer}
  Averaging the misclassification rate over 100 replicates makes the estimate more stable,
  and since we are working with simulated data, we are at liberty to do so.
  For each single replicate, the curve is a noisier version of Figure~\ref{chap16-r_learnbyheart-1}.
\end{answer}
%
Figure~\ref{chap16-r_learnbyheart-1} seems to imply that we can perfectly predict random
labels from random data, if we only fit a complex enough model, i.e., one with many
parameters. How can we overcome such an absurd conclusion?  The problem with the above
code is that the model performance is evaluated on the same data on which it was
trained. This generally leads to positive bias, as you see in this crass example. How can
we overcome this problem? The key idea is to assess model performance on different data
than those on which the model was trained.

%------------------------------------------------------------
\subsection{Cross-validation}\label{sec:supervised:xval}
%------------------------------------------------------------
A naive approach might be to split the data in two halves, and use the first half for
learning (``training''), the second half for assessment (``testing''). It turns out that
this is needlessly variable and needlessly inefficient. Needlessly variable, since by
splitting the data only once, our results can be quite affected by how the splitting
happens to fall. It seems better to do the splitting many times, and average. This will
give us more stable results. Needlessly inefficient, since the performance of machine
learning algorithms depends on the number of samples, and the performance measured on half
the data is likely\footnote{Unless we have such an excess of data that it doesn't matter.}
to be worse than what it is with all the data. For this reason, it is better to use
unequal sizes of training and test data. In the extreme case, we'll use as much as $n-1$
samples for training, and the remaining one for testing.  After we've done this likewise
for all samples, we can average our performance metric. This is called
\eindex{leave-one-out cross-validation}. An alternative is \eindex{$k$-fold
  cross-validation}, where the samples are repeatedly split into a training set of size of
around $n(k-1)/k$ and a test set of size of around $n/k$.  Both alternatives have pros and
contras, and there is not a universally best choice.  An advantage of leave-one-out is that
the amount of data used for training is close to the maximally available data; this is
especially important if the sample size is limiting and ``every little matters'' for the
algorithm.  A drawback of leave-one-out is that the training sets are all very similar, so
they may not sufficiently model the kind of sampling changes to be expected if a new
dataset came along. For large $n$, leave-one-out cross-validation can be needlessly
time-consuming\footnote{See Chapter \textit{Model Assessment and Selection} in the book by
  \citet{HastieTibshiraniFriedman} for further discussion on these trade-offs.}.
%
```{r mclcv, warning = FALSE}
estimate_mcl_loocv = function(x, resp) {
  vapply(seq_len(nrow(x)), function(i) {
    fit  = lda(x[-i, ], resp[-i])
    ptrn = predict(fit, newdata = x[-i,, drop = FALSE])$class
    ptst = predict(fit, newdata = x[ i,, drop = FALSE])$class
    c(train = mean(ptrn != resp[-i]), test = (ptst != resp[i]))
  }, FUN.VALUE = c(0,0)) %>% rowMeans %>% t %>% as_data_frame
}

xmat = matrix(rnorm(n * last(p)), nrow = n)
resp = sample(c("apple", "orange"), n, replace = TRUE)

mcl = lapply(p, function(k) {
  estimate_mcl_loocv(xmat[, 1:k], resp)
}) %>% bind_rows %>% data.frame(p) %>% melt(id.var = "p")

ggplot(mcl, aes(x = p, y = value, col = variable)) + geom_line() +
  geom_point() + ylab("Misclassification rate")
```
%
The result is show in Figure~\ref{chap16-r_mclcv-1}.
%
\begin{marginfigure}
\includegraphics[width=\textwidth]{chap16-r_mclcv-1}
\caption{Cross-validation: the misclassification rate of LDA applied to random data, when
  evaluated on test data that were not used for learning, hovers around 0.5 independent of
  \Robject{p}. The misclassification rate on the training data is also shown. It behaves
  similar to what we already saw in Figure~\ref{chap16-r_learnbyheart-1}.}
\label{chap16-r_mclcv-1}
\end{marginfigure}
%
\begin{ques}
  Why are the curves in Figure~\ref{chap16-r_mclcv-1} more variable (``wiggly'') than in
  Figure~\ref{chap16-r_learnbyheart-1}? How can you overcome this?
\end{ques}
\begin{answer}
  Only one dataset (\Robject{xmat}, \Robject{resp}) was used to calculate
  Figure~\ref{chap16-r_mclcv-1}, whereas for Figure~\ref{chap16-r_learnbyheart-1}, we had
  the data generated within a \Rfunction{replicate} loop. You could similarly extend the
  above code to average the misclassification rate curves over many replicate datasets.
\end{answer}

%------------------------------------------------------------
\subsection{The curse of  dimensionality}\label{sec:supervised:curse}
%------------------------------------------------------------
\begin{marginfigure}
\includegraphics[width=\textwidth]{chap16-r_curseofdim-1}
\caption{As we increase the number of features included in the model, the misclassification
  rate initially improves; as we start including more and more irrelevant features, it increases
  again, as we are fitting noise.}
\label{chap16-r_curseofdim}
\end{marginfigure}
%
In Section~\ref{sec:supervised:xval} we have seen overfitting and cross-validation on
random data, but how does it look if there is in fact a relevant class separation?
%
```{r curseofdim, warning = FALSE, fig.width = 3.5, fig.height = 3}
p   = 2:20
mcl = replicate(100, {
  xmat = matrix(rnorm(n * last(p)), nrow = n)
  resp = sample(c("apple", "orange"), n, replace = TRUE)
  xmat[, 1:6] = xmat[, 1:6] + as.integer(factor(resp))

  lapply(p, function(k) {
    estimate_mcl_loocv(xmat[, 1:k], resp)
  }) %>% bind_rows %>% cbind(p = p) %>% melt(id.var = "p")
}, simplify = FALSE)

mcl = bind_rows(mcl) %>% group_by(p, variable) %>%
   summarise(value = mean(value))

ggplot(mcl, aes(x = p, y = value, col = variable)) + geom_line() +
   geom_point() + ylab("Misclassification rate")
```
%
\begin{marginfigure}
\includegraphics[width=\textwidth]{BiasVarianceTradeoff.png}
\caption{Idealized version of Figure~\ref{chap16-r_curseofdim}, from~\citet{HastieTibshiraniFriedman}.
  A recurrent goal in machine learning is finding the sweet spot in the variance--bias trade-off.}
\label{chap16-BiasVarianceTradeoff}
\end{marginfigure}

The result is shown in Figure~\ref{chap16-r_curseofdim}.
The group centers are the vectors (in $\mathbb{R}^{20}$) given by the
coordinates $(1, 1, 1, 1, 1, 1, 0, 0, 0, \ldots)$ (apples) and
 $(2, 2, 2, 2, 2, 2, 0, 0, 0, \ldots)$ (oranges), and the optimal decision boundary
is the hyperplane orthogonal to the line between them. For $k$ smaller than $6$,
the decision rule cannot reach this hyperplane -- it is
biased. As a result, the misclassification rate is suboptimal, and it decreases with
$k$. But what happens for $k$ larger than $6$? The algorithm is, in principle, able to model the optimal
hyperplane, and it should not be distracted by the additional features. The problem is that
it is. The more additional features enter the dataset, the higher the
probability that one or more of them happen to fall in a way that they \textit{look like}
good, discriminating features in the training data -- only to mislead the classifier and
degrade its performance in the test data. Shortly we'll see how to use penalization to
(try to) control this problem.

The term \eindex{curse of dimensionality} was coined by \citet{Bellman:1961}. It refers to
the fact that high-dimensional spaces are very hard to sample. Our intuitions about
distances between points in a high-dimensionsal space, and the relationship between its
volume and surface, break down.

\begin{ques}
  Assume you have a dataset with 1 000 000 data points in $p$ dimensions. The data are
  uniformly distributed in the unit hybercube (i.\,e., all features lie in the interval
  $[0,1]$). What's the side length of a hybercube that can be expected to contain 10
  points, as a function of $p$?
\end{ques}
\begin{answer}
\begin{marginfigure}
\includegraphics[width=\textwidth]{chap16-r_cursedimans1-1}
\caption{Side length of a hybercube expected to contain 10 points out of 1 million uniformly
  distributed ones, as a function of its dimension $p$. While for $p=1$, this length is
  $10/10^6=10^{-5}$, for larger $p$ it approaches 1, i.\,e., becomes the same as the range
  of each the features. In genomics, we often aim to fit models to data with thousands of
  features.}
\label{chap16-r_cursedimans1-1}
\end{marginfigure}
%
See Figure~\ref{chap16-r_cursedimans1-1}.
%
```{r cursedimans1, fig.width = 3.5, fig.height = 2.5}
sideLength = function(p, pointDensity = 1e6, pointsNeeded = 10)
   (pointsNeeded / pointDensity) ^ (1 / p)
ggplot(tibble(p = 1:750, sideLength = sideLength(p)),
       aes(x = p, y = sideLength)) +
  geom_line(col = "red") + geom_hline(aes(yintercept = 1), linetype = 2)
```
\end{answer}
%
Generally, prediction at the boundaries of feature space is more difficult than in
its interior, as it tends to involve extrapolation, rather than interpolation.
%
\begin{ques}
What fraction of a unit cube's total volume is closer than 0.01 to any of its surfaces, as a
function of the dimension?
\end{ques}
\begin{answer}
\begin{marginfigure}
\includegraphics[width=\textwidth]{chap16-r_cursedimans2-1}
\caption{Fraction of a unit cube's total volume that is in its ``shell'' (here
  operationalised as those points that are closer than 0.01 to its surface) as a function
  of the dimension $p$.}
\label{chap16-r_cursedimans2-1}
\end{marginfigure}
%
See Figure~\ref{chap16-r_cursedimans2-1}.
%
```{r cursedimans2, fig.width = 3.5, fig.height = 2.5}
p = 1:750
volOuterCube = 1 ^ p
volInnerCube = 0.98 ^ p
ggplot(tibble(p = p, `V(shell)` = volOuterCube - volInnerCube),
       aes(x = p, y =`V(shell)`)) + geom_line(col = "blue")
```
\end{answer}
\begin{ques}
What is the coefficient of variation (ratio of standard deviation over average)
of the distance between two randomly picked points in the unit hypercube,
as a function of the dimension?
\end{ques}
\begin{answer}
  \begin{MonteCarlo}{0cm}{}
  \end{MonteCarlo}
We solve this one by simulation. We generate \Robject{n} pairs of random points in the
hypercube (\Robject{x1}, \Robject{x2}) and compute their Euclidean distances.  See
Figure~\ref{chap16-r_cursedimans3-1}. This result can also be
predicted from the central limit theorem.
%
```{r cursedimans3, fig.width = 3.5, fig.height = 2.5}
n = 1000
df = tibble(
  p = round(10 ^ seq(0, 4, by = 0.25)),
  cv = vapply(p, function(k) {
    x1 = matrix(runif(k * n), nrow = n)
    x2 = matrix(runif(k * n), nrow = n)
    d = sqrt(rowSums((x1 - x2)^2))
    sd(d) / mean(d)
  }, FUN.VALUE = NA_real_))
ggplot(df, aes(x = log10(p), y = cv)) + geom_line(col = "orange") +
  geom_point()
```
\begin{marginfigure}[-28mm]
\includegraphics[width=\textwidth]{chap16-r_cursedimans3-1}
\caption{Coefficient of variation (CV)
of the distance between randomly picked points in the unit hypercube,
as a function of the dimension. As the dimension increases, everybody is equally far away from
everyone else: there is almost no variation in the distances any more.}
\label{chap16-r_cursedimans3-1}
\end{marginfigure}
%
\end{answer}

%------------------------------------------------------------
\section{Objective functions}\label{sec:supervised:objective}
%------------------------------------------------------------
We've already seen the \eindex{misclassification rate} (MCR) used to assess our
classification performance in
Figures~\ref{chap16-r_learnbyheart-1}--\ref{chap16-r_curseofdim}. Its population version
is defined as
\begin{equation}\label{sec:super:expmcl}
  \text{MCR} = \text{E}\left[ \mathbbm{1}_{\hat{y} \neq y} \right],
\end{equation}
and for a finite sample
\begin{equation}\label{sec:super:obsmcl}
  \widehat{\text{MCR}} = \frac{1}{n}\sum_{i=1}^n \mathbbm{1}_{\hat{y_i} \neq y_i}.
\end{equation}
This is not the only choice we could make. Perhaps we care more about the
misclassification of apples as oranges than vice versa, and we can reflect this by
introducing weights that depend on the type of error made into the sum of
Equation~\eqref{sec:super:obsmcl} (or the integral of
Equation~\eqref{sec:super:expmcl}). This can get even more elaborate if we have more than
two classes. Often we do not only want to see a single numeric summary, but the whole
\eindex{confusion table}, which in \R{} we can get via expressions like
%
```{r confusiontable, eval=FALSE}
table(truth, response)
```
%
An important special case is binary classification with asymmetric costs -- think about,
say, a medical test. Here, the \eindex{sensitivity} (a.k.a.\ \eindex{true positive rate}
or \eindex{recall}) is related to the misclassification of non-sick as sick, and the
\eindex{specificity} (or \eindex{true negative rate}) depends on the probability of
misclassification of sick as non-sick.  Often, there is a single parameter (e.\,g., a
threshold) that can be moved up and down, allowing a trade-off between sensitivity and
specificity (and thus, equivalently, between the two types of misclassification). In those
cases, we usually are not content to know the classifier performance at one single choice
of threshold, but at many (or all) of them. This leads to \eindex{receiver operating
  characteristic} (\eindex{ROC}) or \eindex{precision-recall} curves.
%
\begin{ques}
  What are the exact relationships between the per-class misclassification rates and
  sensitivity and specificity?
\end{ques}
\begin{answer}
The sensitivity or true positive rate is
\begin{equation*}
\text{TPR} = \frac{\text{TP}}{\text{P}},
\end{equation*}
where $\text{TP}$ is the number of true positives and $\text{P}$ the number of all positives.
The specificity or true negative rate is
\begin{equation*}
\text{SPC} = \frac{\text{TN}}{\text{N}},
\end{equation*}
where $\text{TN}$ is the number of true negatives and $\text{N}$ the number of all negatives.
See also \url{https://en.wikipedia.org/wiki/Sensitivity_and_specificity}
\end{answer}
%
Another cost function can be computed from the \eindex{Jaccard index}, which we already saw
in Chapter~\ref{Chap:6}.
%
\begin{equation}\label{sec:super:jaccard}
  J(A,B) = \frac{|\,A\cap B\,|}{|\,A\cup B\,|},
\end{equation}
where $A$ is the set of samples for which the true class is 1 ($A=\{i\,|\,y_i=1\}$) and
$B$ is the set of samples for which the predicted class is 1. $J$ is a number between 0
and 1, and a high value of $J$ indicates high overlap of the two sets. Note that $J$ does
not depend on the number of samples for which both true and predicted class is 0 -- so it
is particularly suitable for measuring the performance of methods that try to find rare
events.

We can also consider probabilistic class predictions, which come in the form
$\hat{P}(Y\,|\,X)$.  In this case, a possible risk function would be obtained by looking
at distances between the true probability distribution and the estimated probability
distributions. For two classes, the finite sample version of the $\log \text{loss}$ is
\begin{equation}\label{sec:super:logloss}
\log \text{loss} = -\frac{1}{n}\sum_{i=1}^n y_i\log(\hat{p}_i) + (1 - y_i)\log(1 - \hat{p}_i),
\end{equation}
where $\hat{p}_i \in [0,1]$ is the prediction, and $y_i\in\{0,1\}$ is the truth\footnote{Note that
the $\log \text{loss}$ will be infinite if a prediction is totally confident ($\hat{p}_i$
is exactly $0$ or $1$) but wrong.}.

For continuous continuous response variables (regression), a natural choice is the
\eindex{mean squared error} (\eindex{MSE}). It is the average squared error,
\begin{equation}\label{sec:super:obsmse}
 \widehat{\text{MSE}} = \frac{1}{n}\sum_{i=1}^n ( \hat{Y}_i - Y_i )^2.
\end{equation}
The population version is defined analogously, by turning the summation into an integral
as in Equations~\eqref{sec:super:expmcl} and \eqref{sec:super:obsmcl}.

Statisticians call functions like
Equations~(\ref{sec:super:expmcl}--\ref{sec:super:obsmse}) variously (and depending on
context and predisposition) \eindex{risk function}, \eindex{cost function},
\eindex{objective function}\footnote{There is even an \R{} package dedicated to evaluation
  of statistical learners called \CRANpkg{metrics}.}.

\clearpage
%------------------------------------------------------------
\section{Variance--bias trade-off}
%------------------------------------------------------------
\begin{marginfigure}[-5mm]
\includegraphics[width=0.6\textwidth]{TargetBias.png}\\
\includegraphics[width=0.6\textwidth]{TargetVariance.png}
\caption{In the upper bull's eye, the estimates are systematically off target, but in a
  quite reproducible manner. The green segment represents the bias. In the lower bull's
  eye, the estimates are not biased, as they are centered in the right place, however they
  have high variance. We can distinguish the two scenarios since we see the result from
  many shots. If we only had one shot and missed the bull's eye, we could not easily know
  whether that's because of bias or variance.}
\label{chap16:fig:bullseye}
\end{marginfigure}
%
An important fact that helps us understand the tradeoffs when picking a
statistical learning model is that the MSE is the sum of two terms, and often the
choices we can make are such that one of those terms goes down while the other one goes up.
The bias measures how different the average of all the different estimates is from the
truth, and variance, how much an individual one might scatter from the average value
(Figure~\ref{chap16:fig:bullseye}). In applications, we often only get one shot,
therefore being reliably almost on target can beat being right on the long term average
but really off today.  The decomposition
\begin{equation}\label{sec:super:biasvariancedecompose}
  \text{MSE} = \underbrace{\text{Var}(\hat{Y})}_{\text{variance}}
             + \underbrace{\mathbb{E}[\hat{Y}-Y]^2}_{\text{bias}}
\end{equation}
follows by straightforward algebra.

When trying to minimize the MSE, it is important to remember that sometimes we can pay the
price of some bias to obtain a much smaller variance and thus an overall estimator of
lower MSE. In classification (with categorical response variables), different objective
functions than the MSE are used, and there is usually no such straightforward
decomposition as in Equation~\eqref{sec:super:biasvariancedecompose}. In general, we can
go much further in classification applications than in regression with trading biases for variance, since the
discreteness of the response neutralizes certain biases \citep{friedmanbiasvariance01}.

%------------------------------------------------------------
\subsection{Penalization}
%------------------------------------------------------------
In high-dimensional statistics, we are constantly plagued by variance: there is just not
enough data to fit all the possible parameters.
One of the most fruitful ideas in high-dimensional statistics is \eindex{penalization}: a
tool to actively control and exploit the variance-bias tradeoff.

Although generalisation of LDA to high-dimensional settings is possible
\citep{clemmensen2012sparse,witten2011penalized}, it turns out that logistic regression is
a more general approach\footnote{It fits into the framework of generalized linear
  models.}, and therefore we'll now switch to that, using the \CRANpkg{glmnet} package.

Multinomial\footnote{Or, for the special case of two classes, binomial logistic regression.}
logistic regression models the posterior log-odds between $k$ classes and can be written
in the form\footnote{See~\citep{HastieTibshiraniFriedman} for a complete presentation.}
%
\begin{equation}\label{super:eq:deflogregress}
\log \frac{P(Y=i\,|\,X=x)}{P(Y=k\,|\,X=x)} = \beta^0_i + \beta_i x,
\end{equation}
where $i=1,\ldots,k-1$;\; $x$ is the $n\times p$ data matrix ($n$: number of samples, $p$:
number of features), and $\beta_i$ is a $p$-dimensional vector that determines how the
classification odds for class $i$ versus class $k$ depend on $x$. The numbers $\beta^0_i$ are
intercepts and depend, among other things, on the classes' prior probabilities.
Instead of the log odds~\eqref{super:eq:deflogregress} (i.\,e., ratios of class probabilities),
we can also write down an equivalent model for the class probabilities themselves,
and the fact that we here used the $k$-th class as a reference is an arbitrary choice,
as the model estimates are equivariant under this choice~\citep{HastieTibshiraniFriedman}.
The model is fit by maximising the log-likelihood $\mathcal{l}(\beta, \beta^0; x)$,
where $\beta=(\beta_1,\ldots,\beta_{k-1})$ and analogously for $\beta^0$.

So far, so good. But as $p$ gets larger, there is an increasing chance that some of the
estimates go wildly off the mark, due to random sampling happenstances in the data. This
is true even if for each individual coordinate of the vector $\beta_i$, the error
distribution is bounded: the probabilty of there being one coordinate that is in the far
tails increases the more coordiates there are, i.e., the larger $p$ is.

A related problem can also occur, not in \eqref{super:eq:deflogregress}, but in other,
non-linear models, as the model dimension $p$ increases while the sample size $n$ remains
the same: the likelihood landscape around its maximum becomes increasingly flat, and the
maximum-likelihood estimate of the model parameters becomes more and more
variable. Eventually, the maximum is no longer a point, but a submanifold, and the maximum
likelihood estimate is unidentifiable.
% We will see an instance of this in Section~\ref{sec:ML:nonlinear}.

Both of these limitations can be overcome with a modification of the objective: instead of
maximising the bare log-likelihood, we maximise a penalized version of it,
%
\begin{equation}\label{super:eq:penll}
\hat{\beta}= \arg\max_\beta \mathcal{l}(\beta, \beta^0; x) + \lambda \operatorname{pen}(\beta),
\end{equation}
%
where $\lambda\ge0$ is a real number,
and $\operatorname{pen}$ is a convex function, called
the \eindex{penalty function}. Popular choices are $\operatorname{pen}(\beta)=|\beta|^2$
(\eindex{ridge regression}) and $\operatorname{pen}(\beta)=|\beta|^1$
(\eindex{lasso})\footnote{Here, $|\beta|^\nu=\sum_i \beta_i^\nu$ is the $L_\nu$-norm of the
  vector $\beta$. Variations are possible, for instead we could include in this summation
  only some but not all of the elements of $\beta$; or we could scale different elements differently,
  for instance based on some prior belief of their scale and importance.}.
In the \eindex{elastic net}, ridge and lasso are hybridized by
using the penalty function $\operatorname{pen}(\beta)=(1-\alpha)|\beta|^1+\alpha|\beta|^2$
with some further parameter $\alpha\in[0,1]$.
The crux is, of course, how to choose the right $\lambda$, and we will discuss that in the following.

%------------------------------------------------------------
\subsection{Example: predicting colon cancer from stool microbiome composition}
%------------------------------------------------------------
\citet{Zeller:MSB:2014} studied metagenome sequencing data from fecal samples of 156
humans that included colorectal cancer patients and tumor-free controls. Their aim was to
see whether they could identify biomarkers (presence or abundance of certain taxa) that
could help with early tumor detection. The data are available from
\href{https://www.bioconductor.org}{Bioconductor} through its \eindex{ExperimentHub}
service under the identifier EH359.
%
```{r colon1, results = "hide"}
library("ExperimentHub")
eh = ExperimentHub()
zeller = eh[["EH361"]]
```
%
% previously this was:
%    zeller = eh[["EH359"]]
% but in his email of 10.11.2016, 11:10h Georg Zeller recommended:
%  "for now, I’d use
%    exprs(ZellerG_2014.metaphlan_bugs_list.stool())
%  for the purpose of your book chapter."
%
```{r colon1b}
zeller$disease %>% table
```
%$
\begin{ques}
Explore the \Robject{eh} object to see what other datasets there are.
\end{ques}
\begin{answer}
~
```{r exploreeh, results = "hide"}
eh
```
\end{answer}
%
For the following, let's focus on the normal and cancer samples and set the adenomas aside.
%
```{r colon2}
zellerNC = zeller[, zeller$disease %in% c("n", "cancer")]
```
%$
```{r ehzellertest, echo = FALSE}
stopifnot(is.numeric(exprs(zellerNC)), !any(is.na(exprs(zellerNC))))
```
%
Before jumping into model fitting, it is always a good idea to do some exploration of the data.
First, let's look at the sample annotations for some of the samples. We pick them randomly, since
this can be more representative of the whole dataset than only looking at the first or last ones.
%
```{r zellerpData}
pData(zellerNC)[ sample(ncol(zellerNC), 3), ]
```
%
Next, let's explore the feature names\footnote{We define the helper function \Rfunction{formatfn}
  to line wrap these long character strings for the available space here.}.
%
```{r zellerpData}
formatfn = function(x)
   gsub("|", "| ", x, fixed = TRUE) %>% lapply(strwrap)

rownames(zellerNC)[1:4]
rownames(zellerNC)[nrow(zellerNC) + (-2:0)] %>% formatfn
```
%
As you can see, the features are a mixture of abundance quantifications at different
taxonomic levels, from \emph{k}ingdom over \emph{phylum} to \emph{s}pecies. We could select
only some of these, but here we continue with all of them. Next, let's look at the distribution
of some of the features. Here, we show two; in practice, it is helpful to scroll through many
such plots quickly to get an impression.
%
```{r zellerHist, fig.width = 3, fig.height = 4}
ggplot(melt(exprs(zellerNC)[c(510, 527), ]), aes(x = value)) +
    geom_histogram(bins = 25) +
    facet_wrap( ~ Var1, ncol = 1, scales = "free")
```
%
\begin{marginfigure}
\includegraphics[width=\linewidth]{chap16-r_zellerHist-1}
\caption{Histograms of the distributions for two randomly selected features. The distributions are highly
skewed, with many zero values and a thin, long tail of non-zero values.}
\label{chap16:fig:zellerhist}
\end{marginfigure}
%
% Due to the large dynamic range and the skewed distributions of the feature values (across samples),
% it is advantageous to apply a transformation. Here we pick the inverse hyperbolic sine,
% \Rfunction{asinh}\footnote{This choice is motivated by \citet{sagmb2003}. The identity
%  $\text{asinh}(x) = \log(x+\sqrt{x^2+1})$ holds, and this means that for large $x$,
%  $\text{asinh(x)}$ is approximately the same as $\log(x)+\log 2$, while for small x
%  (around zero), it is well approximately by the linear function with slope 1, $f(x)=x$.}.
%
% ```{r colonTrsf}
% zellerTrsf = zellerSub %>% `exprs<-`(., asinh(exprs(.)))
% ```
%
In the simplest case, we fit model~\eqref{super:eq:deflogregress} as follows.
%
```{r glmnet}
library("glmnet")
glmfit = glmnet(x = t(exprs(zellerNC)),
                y = factor(zellerNC$disease),
                family = "binomial")
```
%
A remarkable feature of the \Rfunction{glmnet} function is that it fits
\eqref{super:eq:deflogregress} not only for one choice of $\lambda$, but for
all possible $\lambda$s at once. For now, let's look at the prediction performance
for, say, $\lambda=0.04$. The name of the function parameter is \Robject{s}:
%
```{r colonPred}
predTrsf = predict(glmfit, newx = t(exprs(zellerNC)),
                   type = "class", s = 0.04)
table(predTrsf, zellerNC$disease)
```
%
Not bad\footnote{But remember that this is on the training data, without
  cross-validation.}.
Let's have a closer look at \Robject{glmfit}. The \CRANpkg{glmnet}
package offers a a diagnostic plot that is worth looking at (Figure~\ref{chap16-r_plotglmfit-1}).
%
```{r plotglmfit, fig.width = 4, fig.height = 4}
plot(glmfit, col = brewer.pal(12, "Set3"), lwd = sqrt(3))
```
%
\begin{marginfigure}
\includegraphics[width=\linewidth]{chap16-r_plotglmfit-1}
\caption{Regularization paths for \Robject{glmfit}.}
\label{chap16-r_plotglmfit-1}
\end{marginfigure}
%
\begin{ques}
  What is the $x$-axis in Figure~\ref{chap16-r_plotglmfit-1}?  What are
  the different lines?
\end{ques}
\begin{answer}
  Consult the manual page of the function \Rfunction{plot.glmnet} in
  the \CRANpkg{glmnet} package.
\end{answer}

Let's get back to the question of how to choose the parameter $\lambda$.
We could try many different choices --and indeed, all possible choices-- of
$\lambda$, assess classification performance in each case using cross-validation,
and then choose the best $\lambda$\footnote{You'll already realize from the
  description of this strategy that if we optimize $\lambda$ in this way, the
  resulting apparent classification performance will likely be exaggerated. We need a truly
  independent dataset, or at least another, outer cross-validation loop to get a more
  realistic impression of the generalizability. We will get back to this question at the
  end of the chapter.}. We
could do so by writing a loop as we did in the \Rfunction{estimate\_mcl\_loocv} function
in Section~\ref{sec:supervised:xval}. It turns out that the \CRANpkg{glmnet} package already has
built-in functionality for that, with the function \Rfunction{cv.glmnet}, which we can use instead.
%
```{r colonCV, fig.width = 4, fig.height = 4}
cvglmfit = cv.glmnet(x = t(exprs(zellerNC)),
                     y = factor(zellerNC$disease),
                     family = "binomial")
plot(cvglmfit)
```
%$
The diagnostic plot is shown in Figure~\ref{chap16:fig:colonCV}.
We can access the optimal value with
%
```{r lambda.min}
cvglmfit$lambda.min
```
%$
As this value results from finding a minimum in an estimated curve, it turns out that it
is often too small, i.\,e., that the implied penalization is too weak.  A heuristic
recommended by the authors of the \CRANpkg{glmnet} package is to use a somewhat larger
value instead, namely the largest value of $\lambda$ such that the performance measure is
within 1 standard error of the minimum.
%
\begin{marginfigure}[10mm]
\includegraphics[width=\textwidth]{chap16-r_colonCV-1}
\caption{Diagnostic plot for \Rfunction{cv.glmnet}: shown is a measure of cross-validated
  prediction performance, the deviance, as a function of $\lambda$.
  The dashed vertical lines show \Robject{lambda.min} and \Robject{lambda.1se}.}
\label{chap16:fig:colonCV}
\end{marginfigure}
%
```{r lambda.1se}
cvglmfit$lambda.1se
```
%$

\begin{ques}
  How does the confusion table look like for $\lambda=\;$\Robject{lambda.1se}?
\end{ques}

\begin{answer}
~
```{r predictwithlambda1se}
s0 = cvglmfit$lambda.1se
predict(glmfit, newx = t(exprs(zellerNC)),type = "class", s = s0) %>%
    table(zellerNC$disease)
```
\end{answer}
%

\begin{ques}
What features drive the classification?
\end{ques}

\begin{answer}
~
```{r zellercoef}
coefs = coef(glmfit)[, which.min(abs(glmfit$lambda - s0))]
topthree = order(abs(coefs), decreasing = TRUE)[1:3]
as.vector(coefs[topthree])
formatfn(names(coefs)[topthree])
```
\end{answer}

\begin{ques}
How do the results change if we transform the data, say, with the \Rfunction{asinh} transformation
as we saw in Chapter~\ref{Chap:Clustering}?
\end{ques}
\begin{answer}
See Figure~\ref{chap16:fig:colonCVQues}.
%
```{r colonCVTrsf, fig.width = 4, fig.height = 4}
cv.glmnet(x = t(asinh(exprs(zellerNC))),
          y = factor(zellerNC$disease),
          family = "binomial") %>% plot
```
%$
\begin{marginfigure}
\includegraphics[width=\linewidth]{chap16-r_colonCVTrsf-1}
\caption{like Figure~\ref{chap16:fig:colonCV}, but using an $\text{asinh}$ transformation of the data.}
\label{chap16:fig:colonCVQues}
\end{marginfigure}
\end{answer}

\begin{ques}
  Would a good classification performance on these data mean that this assay is ready for screening and early
  cancer detection?
\end{ques}
\begin{answer}
  No. The performance here is measured on a set of samples in which the cases have similar
  prevalence as the controls. This serves well enough to explore the biology.
  However, in a real-life application, the cases will be much less frequent.
  To be practically useful, the assay must have a much higher specificity, i.\,e., not wrongly
  diagnose disease where there is none. To establish specificity, a much larger set of
  normal samples need to be tested.
\end{answer}


%----------------------------------------------------------------------------
\subsection{Example: classifying mouse cells from their expression profiles}
%-----------------------------------------------------------------------------
Figures~\ref{chap16:fig:colonCV} and \ref{chap16:fig:colonCVQues} are textbook examples of
how we expect the dependence of (cross-validated) classification performance versus model
complexity ($\lambda$) to look. Now let's get back to the mouse embryo cells data. We'll try to
classify the cells from embryonic day \Robject{E3.25} with respect to their genotype.
%
```{r mousecvglmfit, fig.width = 4, fig.height = 4}
sx = x[, x$Embryonic.day == "E3.25"]
embryoCellsClassifier = cv.glmnet(t(exprs(sx)), sx$genotype,
                family = "binomial", type.measure = "class")
plot(embryoCellsClassifier)
```
%
\begin{marginfigure}
\includegraphics[width=\textwidth]{chap16-r_mousecvglmfit-1}
\caption{Cross-validated misclassification error versus penalty parameter for the mouse cells data.}
\label{chap16-r_mousecvglmfit-1}
\end{marginfigure}
%
```{r checkclaimMouseCellsClassifier, echo = FALSE}
stopifnot(sum((diff(embryoCellsClassifier$cvm) * diff(embryoCellsClassifier$lambda)) < 0) <= 2)
```
In Figure~\ref{chap16-r_mousecvglmfit-1} we see that the misclassification error is (essentially) monotonously
increasing with $\lambda$, and is smallest for $\lambda\to 0$, i.\,e., if we apply no penalization at all.
%
\begin{ques}
What is going on with these data?
\end{ques}
\begin{answer}
It looks that inclusion of more, and even of all features, does not harm the classification
performance. In a way, these data are ``too easy''. Let's do a
$t$-test for all features:
```{r mousecellsrowttst, fig.width = 4, fig.height = 2.5}
mouse_de = rowttests(sx, "genotype")
ggplot(mouse_de, aes(x = p.value)) +
  geom_histogram(boundary = 0, breaks = seq(0, 1, by = 0.01))
```
%
The result, shown in Figure~\ref{chap16-r_mousecellsrowttst-1}, shows that large number
of genes are differentially expressed, and thus informative for the class distinction.
We can also compute the pairwise distances between all samples, using all features.
%
```{r mousecellsnn1}
dists = as.matrix(dist(scale(t(exprs(x)))))
diag(dists) = +Inf
```
%
\begin{marginfigure}
\includegraphics[width=\textwidth]{chap16-r_mousecellsrowttst-1}
\caption{Histogram of p-values for the per-feature $t$-tests between genotypes in the E3.25 samples.}
\label{chap16-r_mousecellsrowttst-1}
\end{marginfigure}
%
and then for each sample determine the class of its nearest neighbor
%
```{r mousecellsnn2}
nn = sapply(seq_len(ncol(dists)), function(i) which.min(dists[, i]))
table(x$sampleGroup, x$sampleGroup[nn]) %>% `colnames<-`(NULL)
```
%
Using all features, the nearest neighbour classifier is correct in almost all cases,
including for the E3.25 wildtype vs FGF4-KO distinction. This means
that for these data, there is no apparent benefit in regularisation or feature
selection. Limitations of using all features might become apparent with truly new
data, but that is out of reach for cross-validation.
\end{answer}


%------------------------------------------------------------
\section{A large choice of methods}\label{sec:ML:caret}
%------------------------------------------------------------
We have now seen three classification methods: linear discriminant analysis
(\Rfunction{lda}), quadratic discriminant analysis (\Rfunction{qda}) and the elastic net
(\Rfunction{glmnet}). In fact, there are hundreds of different learning
algorithms\footnote{For an introduction to the subject that uses R and provides many
  examples and exercises, we recommend \citep{James:2013}.} available in R and its add-on
packages. You can get an overview in the CRAN task view
\href{https://cran.r-project.org/web/views/MachineLearning.html}{Machine Learning \& Statistical Learning}.
Some examples are:
\begin{itemize}
\item Support vector machines: the function \Rfunction{svm} in the package \CRANpkg{e1071};
  \Rfunction{ksvm} in \CRANpkg{kernlab}
\item Tree based methods in the packages \CRANpkg{rpart}, \CRANpkg{tree}, \CRANpkg{randomForest}
\item Boosting methods: the functions \Rfunction{glmboost} and \Rfunction{gamboost} in package
  \CRANpkg{mboost}
\item \Rfunction{PenalizedLDA} in the package \CRANpkg{PenalizedLDA},
\Rfunction{dudi.discr} and \Rfunction{dist.pcaiv} in \CRANpkg{ade4}).
\end{itemize}
%
The complexity and heterogeneity of choices of learning strategies, tuning parameters and
evaluation criteria in each of these packages can be confusing.  You will already have noted
differences in the interfaces of the \Rfunction{lda}, \Rfunction{qda} and
\Rfunction{glmnet} functions, i.\,e., in how they expect their input data to presented and
what they return. There is even greater diversity across all the other packages and
functions.  At the same time, there are common tasks such as cross-validation, parameter
tuning and performance assessment that are more or less the same no matter what specific
method is used.  As you have seen, e.\,g., in our \Rfunction{estimate\_mcl\_loocv}
function, the looping and data shuffling involved leads to rather verbose code.

So what to do if you want to try out and explore different learning algorithms?
Fortunately, there are several projects that provide unified interfaces to the large
number of different machine learning interfaces in R, and also try to provide ``best
practice'' implementations of the common tasks such as parameter tuning and performance
assessment.  The two most well-known ones are the packages \CRANpkg{caret} and
\CRANpkg{mlr}. Here were have a look at \CRANpkg{caret}. You can get a list of supported methods
through its \Rfunction{getModelInfo} function. There are quite a few, here we just show
the first 8.
%
```{r caret1, message = FALSE}
library("caret")
caretMethods = names(getModelInfo())
head(caretMethods, 8)
length(caretMethods)
```
%
We will check out a neural network method, the \Rfunction{nnet} function from the eponymous
package. The \Robject{parameter} slot informs us on the the available tuning
parameters\footnote{They are described in the manual of the \Rfunction{nnet} function.}.
%
```{r caret2}
getModelInfo("nnet", regex = FALSE)[[1]]$parameter
```
%$
Let's try it out.
%
```{r caret3, results = "hide", message  = FALSE}
trnCtrl = trainControl(
  method = "repeatedcv",
  repeats = 3,
  classProbs = TRUE)

tuneGrid = expand.grid(
  size = c(2, 4, 8),
  decay = c(0, 1e-2, 1e-1))

nnfit = train(
  Embryonic.day ~ Fn1 + Timd2 + Gata4 + Sox7,
  data = embryoCells,
  method = "nnet",
  tuneGrid  = tuneGrid,
  trControl = trnCtrl,
  metric = "Accuracy")
```
%
\begin{marginfigure}
\includegraphics[width=\textwidth]{chap16-r_nnfit-1}
\caption{Parameter tuning of the neural net by cross-validation.}
\label{fig:ML:nnfit}
\end{marginfigure}
%
That's quite a mouthful, but the nice thing is that this syntax is standardized and
applies across many different methods. All you need to do specify the name of the method
and the grid of tuning parameters that should be explored via the \Robject{tuneGrid}
argument.

Now we can have a look at the output (Figure~\ref{fig:ML:nnfit}).
%
```{r nnfit, fig.width = 3.75, fig.height = 4.25}
nnfit
plot(nnfit)
predict(nnfit) %>% head(10)
```
%
\begin{ques}
  Will the accuracy that we obtained above for the optimal tuning parameters generalize to
  a new dataset? What could you do to address that?
\end{ques}
\begin{answer}
  No, it is likely to be too optimistic, as we have picked the optimum.  To get a
  somewhat more realistic estimate of prediction performance when generalized, we could
  formalize (into computer code) all our data preprocessing choices and the above
  parameter tuning procedure, and embed this in another, outer cross-validation loop
  \citep{ambroise2002selection}. However, this is likely still not enough, as we
  discuss in the next section.
\end{answer}

%------------------------------------------------------------
\subsection{Method hacking}
%------------------------------------------------------------
In Chapter~\ref{Chap:14} we encountered \emph{p-value hacking}.  A similar phenomenon
exists in statistical learning: given a dataset, we explore various different methods of
preprocessing (such as normalization, outlier detection, transformation, feature
selection), try out different machine learning algorithms and tune their parameters
until we are content with the result. The measured accuracy is likely to be too
optimistic, i.\,e., will not generalize to a new dataset. Embedding as many of
our methodical choices into a computational formalism and having an outer cross-validation
loop (not to be confused with the inner loop that does the parameter tuning) will
ameliorate the problem. But is unlikely to address it completely, since not all our
choices can be formalized.

The gold standard remains validation on truly unseen data. In addition, it is never a bad
thing if the classifier is not a black box but can be interpreted in terms of domain
knowledge. Finally, report not just summary statistics, such as misclassification rates,
but lay open the complete computational workflow, so that anyone (including your future
self) can convince themselves of the robustness of the result or of the influence of the
preprocessing, model selection and tuning choices~\citep{StatisticalProof:Holmes:2016}.

%------------------------------------------------------------
\subsection*{Exercises}
%------------------------------------------------------------
\begin{ex}
  Apply a kernel support vector machine, available in the \CRANpkg{kernlab} package, to
  the \Robject{zeller} microbiome data. What kernel function is best?
\end{ex}

\begin{ex}
  It has been quipped that all classification methods are just refinements of two
  archetypal ideas: discriminant analysis and $k$ nearest neighbors. In what sense might
  that be a useful classification?
\end{ex}
\begin{exans}
  In linear discriminant analysis, we consider our objects as elements of $\mathbb{R}^p$, and the
  learning task is to define regions in this space, or boundary hyperplanes between them,
  which we use to predict the class membership of new objects.  This is archetypal for
  \emph{classification by partition}. Generalizations of linear discriminant analysis
  permit more general spaces and more general boundary shapes.

  In $k$ nearest neighbors, no embedding into a coordinate space is needed, but instead we require
  a distance (or dissimilarity) measure that can be computed between each pair of objects, and the
  classification decision for a new object depends on its distances to the training
  objects and their classes. This is archetypal for \emph{kernel-based} methods.
\end{exans}

\begin{ex}
  Use \Rfunction{glmnet} for a prediction of a continous variable, i.e., for
  regression. Explore the effects of using ridge versus lasso penalty.
\end{ex}
\begin{exans}
  There are infinitely many possibilities here. For instance, you could explore the
  prostate cancer data as in Chapter~3 of \citep{HastieTibshiraniFriedman}; the data are
  available in the CRAN package \CRANpkg{ElemStatLearn}.
\end{exans}

\begin{ex}
  Consider smoothing as a regression and model selection problem. What is the equivalent
  quantity to the penalization parameter $\lambda$ in Equation~\eqref{super:eq:penll}? How
  do you choose it?
\end{ex}
\begin{exans}
  We refer to Chapter~5 of \citep{HastieTibshiraniFriedman}
\end{exans}

\begin{ex}
  \emph{Scale invariance}. Consider a rescaling of one of the features in the (generalized)
  linear model~\eqref{super:eq:deflogregress}. For instance, denote the $\nu$-th column of
  $x$ by $x_{\cdot\nu}$, and suppose that $p\ge2$ and that we rescale
  $x_{\cdot\nu} \mapsto s\, x_{\cdot\nu}$ with some number $s\neq0$. What will happen to the
  estimate $\hat{\beta}$ from Equation~\eqref{super:eq:penll} in (a) the unpenalized case
  ($\lambda=0$) and (b) the penalized case ($\lambda>0$)?
\end{ex}
\begin{exans}
  In the unpenalized case, the estimates will be scaled by $1/s$, so that the resulting
  model is, in effect, the same. In the penalized case, the penalty from the $\nu$-th
  component of $\beta$ will be different. If $|s|>1$, the amplitude of the feature is
  increased, smaller $\beta$-components are required for it to have the same effect in the
  prediction, and therefore the feature is more likely to receive a non-zero and/or larger
  estimate, possibly on the cost of the other features; conversely for $|s|<1$.
\end{exans}

%--------------------------------------------------
% Attic
%--------------------------------------------------

```{r kernelsvm, eval = FALSE, echo = FALSE}
library("kernlab")
kfunction= function(linear =0, quadratic=0)
{  k = function (v,w){ linear*sum((v)*(w)) + quadratic*sum((v^2)*(w^2))}
  class(k) = "kernel"
  return(k) }
subx=subx[,2:3]
svp = ksvm(subx,dftxy$tg,type="C-svc",C = 100, kernel=kfunction(1,0),scaled=c())
plot(c(min(subx[,1]), max(subx[,1])),c(min(subx[,2]), max(subx[,2])),
            type='n',xlab='x1',ylab='x2')
ymat = ymatrix(svp)
points(subx[-SVindex(svp),1], subx[-SVindex(svp),2],
         pch = ifelse(ymat[-SVindex(svp)] < 0, 2, 1))
points(subx[SVindex(svp),1], subx[SVindex(svp),2],
         pch = ifelse(ymat[SVindex(svp)] < 0, 17, 16))

# Extract w and b from the model
w = colSums(coef(svp)[[1]] * subx[SVindex(svp),])
b = b(svp)
# Draw the lines
abline(b/w[2],-w[1]/w[2])
abline((b+1)/w[2],-w[1]/w[2],lty=2)
abline((b-1)/w[2],-w[1]/w[2],lty=2)
```
