---
title: "Basic robustness concepts of statistics"
author: "Vince Carey"
date: "June 14, 2017"
output:
  slidy_presentation: null
  ioslides_presentation: default
---
<style type="text/css">
.slide { font-size: 28px; }

body, td {
   font-size: 28px;
}
code.r{
  font-size: 24px;
  background-color: white;
  font-color: black;
}
pre {
  font-size: 24px;
}
</style>

Road map:
```{r setup,echo=FALSE,results="hide"}
suppressPackageStartupMessages({
library(parody)
library(ggplot2)
library(gridExtra)
library(DESeq2)
})
```

- simulation of test size and power
- effects of sample contamination
- resistant statistics: robustification of key statistical procedures
- measuring influence of observations in DESeq2

# Some code to enter in your workspace

Mystery functions?
```{r getco}
ts_zpm = function(d) sqrt(length(d))*mean(d)/sd(d)
```
```{r gc2}
contam1 = function(x, slip=5) {x[1] = x[1]+slip; x}
```

# Try them out

```{r dotr}
set.seed(12345)
X = rnorm(50)
ts_zpm(X)
ts_zpm(contam1(X))
ts_zpm(contam1(X,100))
```

# The concept of the critical region

- Wolfgang: count events and check whether the count falls into a region prescribed to have low probability under the null hypothesis
- The probabilities of regions are prescribed by the binomial distribution
- We need probabilities and principles for choosing the critical region
    - symmetric about null value?
    - represent a specific direction of effect (one-sided?)
    - based on theory or simulation?

# Assumptions underlying a test procedure
- _Exact_ critical regions based on finite sample distribution of a statistic
    - $t$, Wilcoxon
    - the parent distribution must satisfy certain conditions
        - data are a random sample from homogeneous population (iid)
        - for $t$: parent is Gaussian
        - for Wilcoxon: parent is continuous, finite variance (?)
- _Approximate_ critical regions based on large sample theory
    - how large is large enough?
    - data are a random sample from homogeneous population (iid)
- _Approximate_ critical regions based on simulation
    - the generative distribution accurately represents variation underlying the experiment or observations


# Computing and reporting a t test for a simple hypothesis

```{r lkd}
set.seed(12345)
X = rnorm(50)
tst = t.test(X)
tst
```

# Components of the result

```{r lks}
str(tst)
```

# The structure of the t statistic: a ratio of measures of location and dispersion

```{r lkstat}
tst$statistic  # from R
sqrt(50)*mean(X)/sd(X) # by hand
```

# Simulating the distribution of t under the null hypothesis: $X \sim N(0,\sigma^2)$

```{r dosim, cache=TRUE}
set.seed(12345)
simdist = replicate(10000,  ts_zpm( rnorm(50) ))
head(simdist)
```

# Simulated data and theoretical density

```{r lkh,fig=TRUE}
hist(simdist, freq=FALSE)
lines(seq(-3,3,.01), dt(seq(-3,3,.01), 49))
```

# The one-sided p-value for $\hat{t}$ = 1.1579

simulated: 
```{r simlk}
mean(simdist > 1.1579)
```
theoretical (exact):
```{r doin}
integrate(function(x) dt(x,49), 1.1579, Inf)$value
```

# What if there is a contaminated observation?

```{r simdist2}
contsim = replicate(10000, 
    ts_zpm( contam1( rnorm(50) ) ) )
critval_1sided = qt(.95, 49)
mean(simdist > critval_1sided) # uncontaminated
mean(contsim > critval_1sided) # contaminated
```

# Can we fix this? {.smaller}
```{r robt, cache=TRUE}
library(parody)
robust_t = function(x) {
 outchk = calout.detect(x, method="GESD")
 if (!is.na(outchk$ind[1])) x = x[-outchk$ind]
 sqrt(length(x))*mean(x)/sd(x)
}
set.seed(12345)
contsim_r = replicate(10000, 
    robust_t( contam1( rnorm(50) ) ) )
mean(contsim_r > critval_1sided) # robust test on contaminated
```

# Recap {.smaller}

- the one-sided one sample t test for $H_0: \mu = 0$ involves
    - $n$, $\bar{X}$, $s$ to form the test statistic, and
    - the $t$ density to form critical values
- simulation from the null distribution can be used to obtain an empirical p value
- simulation from the contaminated distribution with a single observation shifted by $5\sigma$ leads to a Type I error rate (using the standard $t$
critical value for $\alpha = 0.05$) of 0.095
- robustification of the test statistic using calibrated outlier removal can restore (approximately) the nominal Type I error rate
    - can't conclude too much from this example, of course ... there is considerable theoretical literature on sensitivity of tests to contamination
    - the t test is described as 'robust' in many texts, referring to 
        - insensitivity to violation of the assumption of Gaussian population 
        - insensitivity to violation of the assumption of equal variances in the two-sample case

# Power to reject $H_0:\mu = 0$ when $\mu = 0.4$

```{r lkp}
power.t.test(n=50, type="one.sample", 
   alt="one.sided", delta=.4)
# empirical
mean( replicate(10000, 
    ts_zpm(rnorm(50, .4))>qt(.95, 49)) )
```

# Effect of contamination: a single large contaminant can slash power

```{r lkloss}
mean( replicate(10000, ts_zpm(
     contam1(rnorm(50, .4), slip=25))>qt(.95, 49)) )
```
Exercises: 

- plot power against size of slip 
- show that calibrated outlier removal can restore power lost with contamination

# Sample median is a resistant estimator of location {.smaller}

We'll use a series of outlier magnitudes (0:10) and summarize distributions of estimators
```{r lkresis, cache=TRUE}
set.seed(12345)
mns <- sapply(0:10, function(o) 
  median(replicate(5000, mean(contam1(rnorm(50),o))))) 
set.seed(12345)
meds <- sapply(0:10, function(o) 
  median(replicate(5000, median(contam1(rnorm(50),o))))) 
```

# Note sensitivity of mean {.smaller}

```{r dopl, fig=TRUE, fig.height=4.5}
plot(0:10, mns, xlab="outlier magnitude", ylab="median of stat",
  pch=19)
points(0:10, meds, pch=19, col="blue")
legend(0, .1, pch=19, col=c("black", "blue"), legend=c("mean", "median"))
```

# Sample MAD is a resistant estimator of dispersion {.smaller}

```{r lkresis2, cache=TRUE}
set.seed(12345)
sds <- sapply(0:10, function(o) 
  median(replicate(5000, sd(contam1(rnorm(50),o))))) 
set.seed(12345)
mads <- sapply(0:10, function(o) 
  median(replicate(5000, mad(contam1(rnorm(50),o))))) 
```

# Note sensitivity of SD  {.smaller}

```{r dopl2, fig=TRUE, fig.height=4.5}
plot(0:10, sds, xlab="outlier magnitude", ylab="median of stat",
  pch=19)
points(0:10, mads, pch=19, col="blue")
legend(0, 1.2, pch=19, col=c("black", "blue"), legend=c("SD", "MAD"))
```

# Recap

- Resistant estimators "peel away" extreme values
- These estimators achieve "high breakdown bound"
    - if up to 50\% of data are corrupted, median continues to estimate population median
    - if up to 25\% of data are corrupted, MAD continues to estimate a scaled SD

# Exercises

- Generalize `contam1()` to help demonstrate the breakdown concept
- Assess the robustness of size and power of rank-based
tests (such as Wilcoxon's signed rank test) to contamination by outliers.
- Note that dsignrank, qsignrank are available.
```{r comq}
qsignrank(.95, 50)
set.seed(12345)
wilcox.test(rnorm(50, .4))$statistic
```

# Deficiencies of scalar summaries

```{r lkans, fig=TRUE, echo=FALSE}
# thanks Neil Saunders, https://rpubs.com/neilfws/91339
library(ggplot2)
library(gridExtra)
p1 <- ggplot(anscombe) + geom_point(aes(x1, y1), color = "darkorange", size = 3) + theme_bw() + scale_x_continuous(breaks = seq(0, 20, 2)) + scale_y_continuous(breaks = seq(0, 12, 2)) + geom_abline(intercept = 3, slope = 0.5, color = "cornflowerblue") + expand_limits(x = 0, y = 0) + labs(title = "dataset 1")
p2 <- ggplot(anscombe) + geom_point(aes(x2, y2), color = "darkorange", size = 3) + theme_bw() + scale_x_continuous(breaks = seq(0, 20, 2)) + scale_y_continuous(breaks = seq(0, 12, 2)) + geom_abline(intercept = 3, slope = 0.5, color = "cornflowerblue") + expand_limits(x = 0, y = 0) + labs(title = "dataset 2")
p3 <- ggplot(anscombe) + geom_point(aes(x3, y3), color = "darkorange", size = 3) + theme_bw() + scale_x_continuous(breaks = seq(0, 20, 2)) + scale_y_continuous(breaks = seq(0, 12, 2)) + geom_abline(intercept = 3, slope = 0.5, color = "cornflowerblue") + expand_limits(x = 0, y = 0) + labs(title = "dataset 3")
p4 <- ggplot(anscombe) + geom_point(aes(x4, y4), color = "darkorange", size = 3) + theme_bw() + scale_x_continuous(breaks = seq(0, 20, 2)) + scale_y_continuous(breaks = seq(0, 12, 2)) + geom_abline(intercept = 3, slope = 0.5, color = "cornflowerblue") + expand_limits(x = 0, y = 0) + labs(title = "dataset 4")

grid.arrange(p1, p2, p3, p4)
```

# Anscombe's data

Show that the (x,y) pairs have identical

    - marginal means
    - marginal SDs
    - correlation coefficients
    - linear regressions of y on x

Use MASS::rlm to get a model for y3, x3 that fits the majority
of points exactly

# Outliers in RNA-seq analysis

- DESeq2 has extensive discussion of Cook's distance for identifying and reducing effects of apparent outliers
- Let's get a feel for what Cook's distance is: source("shi.R"); docook()
 
# Simulating an RNA-seq experiment

```{r lksimd, results="hide"}
suppressMessages({set.seed(12345)
library(DESeq2)
S1 = makeExampleDESeqDataSet(betaSD=.75)
D1 = DESeq(S1)
R1 = results(D1)})
```{r lksum}
summary(R1)
```

# Exercises

- Find the outliers labeled by DESeq2
- Are these really suspect?
- For the homogeneous simulation process demonstrated here, estimate the frequency of outlier labeling by the default rules
- Define a process for injecting aberrant counts and assess the accuracy of the default rules for identifying them.  How do the rules contribute to validity and power of the basic testing procedure?

# Conclusion

- Robustness is commonly cited as a property of new methodologies
    - the term is often used vaguely
- In statistics, there is robustness of validity (Type I error rate is maintained despite failure of certain assumptions) and robustness of efficiency (power does not decline in the presence of failure of certain assumptions)
- _Resistance_ is a related concept: a fraction of _arbitrarily aberrant_ values may be present, but their effect on estimation or inference is bounded
- Outlier labeling is carefully studied for generations; the outliers may be the most interesting points in your data
- Learn how to use simulation, with specific attention to realism and flexibility of implementation, so that you can explore a variety of scenarios
