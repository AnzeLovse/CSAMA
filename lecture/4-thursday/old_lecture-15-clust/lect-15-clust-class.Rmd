---
title: "CSAMA 2018: Clustering, classification, and regression with genomic examples"
author: Vince Carey
date: July 12, 2018
output: 
  ioslides_presentation:
    fig_height: 4.8
runtime: shiny
---
<!--
  ioslides_presentation:
    incremental: false
    fig_height: 3.8
runtime: shiny
-->

```{r setup, echo=FALSE, results="hide"}
suppressPackageStartupMessages({
library(grid)
library(png)
library(MLInterfaces)
library(hgu133a.db)
library(hgu95av2.db)
library(survival)
library(BiocStyle)
library(genefu)
library(drosmap)
library(tissuesGeneExpression)
library(cluster)
library(grid)
library(png)
library(limma)
library(rafalib)
library(fpc)
library(impute)
library(Biobase)
library(tissuesGeneExpression)
library(ggvis)
library(shiny)
})
```

## Useful resources for high-dimensional data analysis in genomics

James, Witten, Hastie, Tibshirani, [Introduction to Statistical Learning](https://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf), a freely available PDF of a Springer monograph, with applications in R

Hastie, Tibshirani and Friedman, [Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf), a freely available PDF of a Springer monograph

Holmes and Huber, [Modern Statistics for Modern Biology](http://web.stanford.edu/class/bios221/book/introduction.html), open HTML of a Cambridge University Press monograph

## Road map
- use cases
- user interface concepts
- cluster analysis components 
    - primitive sensitivity analysis
- classifier components
    - role of metapackages like caret/mlr/MLInterfaces

## Use case 1: Species and organ of origin

<img src="newbarco.png" width=950/>

## Use case 2: Oncotype DX gene signature for breast cancer survival

- 21 genes useful for prediction of breast cancer recurrence
- Paik, Shak, Tang et al. NEJM 2004
- `r Biocpkg("genefu")` package includes notation for the signature (`sig.oncotypedx`)
- We'll consider the capacity of the gene set for predicting overall survival in a classic breast cancer dataset (van de Vijver 2002) as packaged in `genefu`


## Setup for NKI breast cancer expression/clinical data

```{r lk1}
library(genefu); library(survival)
data(nkis)
map = as.character(annot.nkis$NCBI.gene.symbol)
names(map) = as.character(annot.nkis$probe)
ndata.nkis = data.nkis
colnames(ndata.nkis) = map[colnames(data.nkis)]
cbind(ndata.nkis[1:4,1:4], demo.nkis[1:4,5:8])
```

## Label expression columns with appropriate symbols; test

```{r docol}
nkSurv = Surv(demo.nkis$t.os, demo.nkis$e.os)
odata = ndata.nkis[, intersect(as.character(sig.oncotypedx$symbol), 
    colnames(ndata.nkis))]
fullnk = cbind(demo.nkis, odata)
coxph(nkSurv~er+age, data=fullnk)
```

## Create a survival tree using all available clinical and expression data {.smaller}

```{r tree}
rfullnk = fullnk[,-c(1,2,3,9,10,11,12,13,14,17,18,19)]
library(rpart); r1 = rpart(nkSurv~.,data=rfullnk)
r1
```

CRAN package `r CRANpkg("partykit")` enhances tree support in `r CRANpkg("rpart")`
and provides many additional models
```{r prunit}
library(partykit)
p1p = as.party(prune(r1, cp=.05))
```

## Visualize the pruned tree along with K-M curves for leaves

```{r lkpar, fig=TRUE, echo=FALSE}
plot(p1p)
```


## Comments

* *Data type diversity*: discrete factors, 
 censored biomarker values or survival times, missing values
* *Curse of dimensionality:* as the number of features increases, utility
of distance metrics for object grouping diminishes (space is mostly
empty, distances generally small) 
* *Bet on sparsity principle:* favor procedures that are able to prune
features/dimensions, because in non-sparse case, nothing works
* All the results displayed are tunable, could be interactive
* Sensitivity analysis: Enhance the capacity of reports to demonstrate
their own robustness

## Remainder of talk

* Details of some clustering algorithms
* R/Bioconductor strategies: user interface and object designs
* Cluster analysis formalities; hclustWidget
* Classifier formalities; mlearnWidget

## Unsupervised learning/exploratory data analysis

* Basic formalism: $x_{ij}$ is the value of feature $j$ on sample $i$
* Samples are enumerated $i = 1, \ldots, n$, features $j = 1, \ldots, p$
* Clustering: Allocate samples to $C_1, \ldots, C_K$, that are mutually exclusive subsets of $\{1, \ldots, N\}$
* Principle: allocation should lead to clusters whose members are **similar**
* Need a quantification of similarity/dissimilarity for the $x$ in terms of their features


## Distance algebra

![wikipedia](figures_vjc/metricDef.png)


## Examples: 

### Euclidean distance

- High-school analytic geometry: distance between two points in $R^3$
- $p_1 = (x_1, y_1, z_1)$, $p_2 = (x_2, y_2, z_2)$
- $\Delta x = x_1 - x_2$, etc.
- $d(p_1, p_2) = \sqrt{(\Delta x)^2 + (\Delta y)^2 + (\Delta z)^2}$

### Manhattan distance

- $d(p_1, p_2) = |\Delta x| + |\Delta y| + |\Delta z|$

### New concept of distance for categorical vectors:

Sam Buttrey and Lyn Whitaker's `r CRANpkg("treeClust")` ([R Journal article](https://journal.r-project.org/archive/2015-2/buttrey-whitaker.pdf))

## Comparing 20-vectors

![corre](corrEucDist.png)

## Contours (from Holmes, Huber)

<img src="MSMBdistance.png" width=600\>


## Generalized Euclidean (from Holmes, Huber)

<img src="mahalanobis.png" width=700\>

## Algorithm: K-means

* The number of clusters is determined in advance to be $K$
* Initially, data are allocated randomly to $K$ clusters, and
centroids are computed as multivariate means
* Points closest to centroids are allocated to that centroid's cluster,
and centroids are recomputed
* Algorithm iterates to convergence

## Illustration from James, Witten et al.

<img src="kmeansProgress.png" width=550\>

## Allocation sensitivity to initial assignment

<img src="kmeansNonUnique.png" width=550\>

## Hierarchical clustering: building

<img src="treeBuilding.png" width=550\>

## Hierarchical clustering: cutting

<img src="treeCutting.png" width=750\>

## The ward.D2 agglomeration method?
- Enables very rapid update upon change of distance or # genes

```{r wardalg,fig=TRUE,echo=FALSE,fig.height=5.2}
library(png)
im = readPNG("figures_vjc/wards.png")
grid.raster(im)
```

## The Jaccard similarity coefficient?

```{r jacc,fig=TRUE,echo=FALSE,fig.height=5.2}
library(png)
im = readPNG("figures_vjc/jaccdef.png")
grid.raster(im)
```

## On the user interface

* The method is primary (constituents of CRAN task view "MachineLearning")
* What does the learner consume?
    - data in a specific format, tuning parameters
* What does the learner emit?
    - an object with scores, assignments, metadata about the run
* Aims
    - reduce complexity of user tasks
    - capitalize on formal structuring of containers for inputs and outputs
    - foster sensitivity analysis
* We'll now use a modified MLInterfaces::hclustWidget that capitalizes on these notions


```{r sta, echo=FALSE}
data(tissuesGeneExpression)
library(Biobase)
tiss = ExpressionSet(e)
rownames(tab) = colnames(e)
pData(tiss) = tab
tiss = tiss[, tiss$SubType == "normal"]
annotation(tiss) = "hgu133a.db"
etiss = exprs(tiss)
colnames(etiss) = tiss$Tissue
#datatable(pData(tiss))
```

## Exploring clusters with tissue-of-origin data


```{r mkhc,echo=FALSE}
nicehclustWidget = function(mat) {
 shinyApp(ui = fluidPage(
  fluidRow(
   column(3,  numericInput("ngenes", label = "N genes:", 100, min = 2, max = 100000)),
   column(3,  selectInput("distmeth", label = "Distance method:",
               choices = c("euclidean", "maximum", "manhattan",
               "binary"), selected = "euclidean")),

   column(3,  selectInput("fusemeth", label = "Agglomeration method:",
               choices = c("complete", "average", "ward.D2", "single",
                   "median", "centroid"), selected="complete")),
   column(3,  numericInput("numclus", label = "K:", 6, min = 2, max = 9))
          ),
  fluidRow(column(8, plotOutput("tree")), column(4, ggvisOutput("pcp")))
 ), server= function(input, output, session) {
    output$tree <- renderPlot({
dm = dist(mat[,1:input$ngenes], method=input$distmeth)
sink(tempfile())
cb <- clusterboot(dm, clustermethod=hclustCBI, method=input$fusemeth, k=input$numclus, showplots=FALSE, scaling=FALSE)
sink(NULL)
      dend = hclust( dm, method=input$fusemeth )
      par(mar=c(3,3,3,1))
      mnbc = round(rmnbc <- mean(cb$bootmean),2)
      plot(dend, main=paste0("Boot. Jacc. at k=", input$numclus, ": ",
        paste("mean: ", mnbc, collapse=""), "; resids:",
        paste(round(rmnbc - cb$bootmean,2), collapse=", ")), xlab=" ")
    })
    P1 <- reactive({
           all_values <- function(x) {
             if(is.null(x)) return(NULL)
             row <- pcdf[pcdf$rowid == x$rowid, ]
             paste0(names(row), ": ", format(row), collapse = "<br />")
           }

      pc = prcomp(mat[,1:input$ngenes])$x
      dm = dist(mat[,1:input$ngenes], method=input$distmeth)


      dend = hclust( dm, method=input$fusemeth )
      ct = cutree(dend, k=input$numclus)
      pcdf = data.frame(PC1=pc[,1], PC2=pc[,2], tiss=pData(tiss)$Tissue,
         rowid=1:nrow(pc), assigned=factor(ct))
      pcdf %>% ggvis(~PC1, ~PC2, key := ~rowid, fill = ~assigned) %>% layer_points() %>%
               add_tooltip(all_values, "hover") 
#      pairs(pc[,1:3], col=ct, pch=19, cex=1.5)
      }) 
      P1 %>% bind_shiny("pcp")
} )
}
```

```{r lkwid, echo=FALSE}
nicehclustWidget(t(etiss))
```


## Summary

* Hierarchical clustering is tunable; distance, fusion
method, feature selection all have impact
* There are other principles/algorithms: divisive, semi-supervised, model-based
* Other figures of merit: consensus, gap statistic
* See the `r CRANpkg("mlr")` for structured interface


## On classification methods with genomic data

- Vast topic
- Key resources in R:
    - Machine Learning [task view](http://cran.r-project.org/web/views/MachineLearning.html) at CRAN
    - 'metapackage' [mlr](http://cran.r-project.org/web/packages/mlr/index.html)
- In Bioconductor, consider
    - The 'StatisticalMethod' task view (next slide)
    - MLInterfaces (a kind of metapackage)

## BiocViews: StatisticalMethod

```{r statmv,fig=TRUE,echo=FALSE,fig.height=4.6}
library(png)
im = readPNG("figures_vjc/statmeth.png")
grid.raster(im)
```

## Conceptual basis for methods covered in the talk

- "Two cultures" of statistical analysis (Leo Breiman)
    - model-based 
    - algorithmic

- Ideally you will understand and use both
    - $X \sim N_p(\mu, \Sigma)$, seek and use structure in $\mu$, $\Sigma$ as estimated from data; pursue weakening of model assumptions
    - $y \approx f(x)$ with response $y$ and features $x$, apply agnostic algorithms to the data to choose $f$ and assess the quality of the prediction/classification

## Linear discriminant analysis

- Use a linear combination of features to define a score for each object 
- The value of the score determines the class assignment
- This assumes that the features are quantitative and are measured consistently for all objects 
- for $p$-dimensional feature vector $x$ with prior probability $\pi_k$, mean $\mu_k$ for class $k$, and
common covariance matrix for all classes
$$
\delta_k(x) = x^t\Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^t \Sigma^{-1} \mu_k + \log \pi_k
$$
is the discriminant function; $x$ is assigned to the class for which $\delta_k(x)$ is largest

## Other approaches, issues

- Direct "learning" of statistical parameters in regression or
neural network models
- Recursive partitioning of classes, repeating searches through all
features for optimal discrimination
- Ensemble methods in which votes are assembled among different learners
or over perturbations of the data
- Unifying loss-function framework: see _Elements of statistical learning_ by
Hastie, Tibshirani and Friedman
- Figures of merit: misclassification rate (cross-validated), AUROC 

## A demonstration with tissue-of-origin expression data follows

##

```{r lkmlw,echo=FALSE}
mlearnWidget(tiss, infmla=Tissue~.)
```

## Remarks

* all examples here employ mature, reduced data
* statistical learning also important at early stages, but data volume
leads to challenges
* interactive modeling/learning as the product
   - in opposition to a potentially overoptimistic selection
* new work on post-selection inference in `r CRANpkg("selectiveInference")`


<!--
3) components of a cluster analysis: vectors (quantitative or
   categorical), feature selection/weighting,
   distance metric, amalgamation or partitioning
   algorithm, figure of merit, assignments/scores

4) hclustWidget -- a tool for limited sensitivity analysis

5) components of a classifier: training set (vectors with class
   labels), feature selection procedure, selection of tuning parameters,
   algorithm execution, training assignments, deployment on test
   data, evaluation of figure of merit


Some useful software
  - Bioconductor MLInterfaces lets you explore various approaches with
genomic data fairly conveniently
  - more recently, Bischl's mlr package comprehensively "metapackages"
CRAN learners and more
  - we've already had a reference to caret, another metapackaging package

Some formalism for hierarchical clustering
   - distance
   - Ward's D2 and iterative agglomeration
   - cutting the tree for labeling purposes

Projecting the labeled data
   - PCA
   - biplots

Some formalism for tree-based learning
   - CART: recursive partitioning to maximize node purity
   - cost-complexity pruning: minimize error rate plus penalty for tree size
   - classifier: drop a vector down the fitted tree
   - random forests: ensembles of trees created through resampling, voting
       to create assignments
   - classifier: drop a vector down all the trees and tally votes

-->
